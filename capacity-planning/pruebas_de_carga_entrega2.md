# Reporte pruebas de carga

## Escenario 1 Sanidad (Smoke Test)
### Configuraci√≥n del Escenario
* Tipo de prueba: Smoke Test (Sanidad)
* Usuarios concurrentes: 5
* Duraci√≥n: 1 minuto
* Prop√≥sito: Validaci√≥n b√°sica de funcionalidad y telemetr√≠a

---
### Objetivo de la Prueba

Validar la **disponibilidad**, **respuesta b√°sica** y **estabilidad inicial** del endpoint desplegado en AWS bajo una carga m√≠nima (prueba de humo), garantizando que el sistema est√° operativo y puede manejar peticiones HTTP sin errores antes de realizar pruebas de carga m√°s exigentes.

---

### M√©tricas Clave Obtenidas

| M√©trica | Valor | Interpretaci√≥n |
|----------|--------|----------------|
| **# de Muestras** | 86 | Total de peticiones ejecutadas |
| **Media (Tiempo de respuesta)** | **3,376 ms (‚âà 3.4 s)** | Tiempo promedio de respuesta |
| **M√≠nimo** | 1,291 ms | Mejor tiempo de respuesta observado |
| **M√°ximo** | 8,790 ms | Tiempo m√°ximo observado |
| **Desviaci√≥n est√°ndar** | 1,620 ms | Variabilidad alta en los tiempos de respuesta |
| **% de Error** | 0.00 % | Todas las peticiones fueron exitosas |
| **Rendimiento (Throughput)** | 1.3 req/seg (‚âà 80 req/min) | Promedio de peticiones procesadas por segundo |
| **Media de Bytes** | 552 bytes | Tama√±o medio de la respuesta HTTP |
| **Sent KB/sec** | 4,156 KB/s | Volumen de datos enviados por segundo |
| **Kb/sec recibidos** | 0.73 KB/s | Volumen de datos recibidos por segundo |

---

### Comportamiento del Sistema

- El **sistema respondi√≥ correctamente** a todas las solicitudes (0% de error).
- El **tiempo promedio de respuesta (3.3 s)** es aceptable para una prueba de humo, aunque algo elevado para endpoints simples.
- El rendimiento fue estable en general, sin evidencia de fallas o interrupciones.

---

### Identificaci√≥n de Posibles Cuellos de Botella

1. **Alta desviaci√≥n est√°ndar (¬±1620 ms)** ‚Üí indica fluctuaciones importantes entre las respuestas.
2. **Picos de hasta 8.7 s** podr√≠an deberse a:
   - Sobrecarga moment√°nea del backend o tareas sincr√≥nicas costosas.
   - Latencia de red entre el cliente JMeter y el endpoint.
3. **Throughput relativamente bajo (1.3 req/seg)** podr√≠a estar limitado por:
   - Configuraci√≥n del test (n√∫mero de hilos, ramp-up time).
   - Tiempos de respuesta altos que reducen el flujo de peticiones concurrentes.

---

### Recomendaciones de Mejora

1. **Optimizaci√≥n del Backend**
   - Revisar consultas a bases de datos y operaciones sincr√≥nicas.
   - Implementar cach√© o asincron√≠a en puntos cr√≠ticos.
2. **Revisi√≥n de la Infraestructura AWS**
   - Verificar cold starts y ajustar pol√≠ticas de escalado autom√°tico.
   - Revisar configuraci√≥n de instancias o Lambdas (memoria, CPU, keep warm).
3. **Monitoreo Complementario**
   - Registrar m√©tricas de CPU, memoria y tiempos de procesamiento del backend.
   - Correlacionar estos datos con los tiempos de respuesta de JMeter.

---

### Conclusi√≥n

El endpoint **responde correctamente** a todas las peticiones y mantiene **disponibilidad del 100%**, por lo que el escenario **Sanidad (Smoke)** se considera **aprobado**.  
Sin embargo, los **tiempos de respuesta y su variabilidad** sugieren que antes de pruebas de carga o estr√©s se debe:
- Identificar causas de los picos,
- Optimizar la latencia promedio,
- Afinar la configuraci√≥n del entorno AWS.

---

### Estado Final del Sistema

| Estado | Descripci√≥n |
|--------|--------------|
| ‚úÖ **Aprobado con observaciones** | El sistema es funcional y estable para pruebas iniciales, pero se recomienda optimizaci√≥n de rendimiento antes de escalar las pruebas. |

---

### Evidencias JMeter
### Configuracion inicial
![E1_configuracion_inicial](img/E1_configuracion_inicial.png)
### Reporte
![E1_reporte](img/E1_aws_reporte.png)

---


## Escenario 2: Pruebas de Desempe√±o - Escalamiento R√°pido (Ramp 100 usuarios)

### Configuraci√≥n del Escenario

| Par√°metro | Valor |
|------------|--------|
| **Tipo de prueba** | Escalamiento r√°pido (Ramp-up) |
| **Usuarios m√°ximos** | 100 usuarios |
| **Periodo de subida (Ramp-up)** | 180 segundos (3 minutos) |
| **Duraci√≥n total** | 480 segundos (8 minutos) |
| **Ejecuci√≥n** | Bucle continuo hasta completar la duraci√≥n total |

---

### Objetivo de la Prueba
Evaluar el **comportamiento y estabilidad del sistema** bajo un **incremento progresivo y r√°pido de usuarios concurrentes**, identificando el punto en el que el servicio comienza a degradarse en rendimiento o presenta errores.  
El objetivo es determinar si el sistema puede **mantener tiempos de respuesta aceptables y estabilidad** con 100 usuarios concurrentes en un entorno AWS.

---

### M√©tricas Clave Obtenidas

| M√©trica | Valor | Interpretaci√≥n |
|----------|--------|----------------|
| **# de Muestras** | 991 | Total de solicitudes procesadas durante la prueba |
| **Media (Tiempo de respuesta)** | **42,124 ms (‚âà 42.1 s)** | Tiempo promedio de respuesta por petici√≥n |
| **Mediana** | 45,472 ms | El 50% de las peticiones respondieron en menos de 45 s |
| **Percentil 90** | 56,644 ms | El 90% de las respuestas fueron menores a 56.6 s |
| **Percentil 95** | 61,752 ms | El 95% de las respuestas fueron menores a 61.7 s |
| **Percentil 99** | 88,866 ms | El 99% respondi√≥ antes de 88.8 s |
| **M√≠nimo** | 1,309 ms | Mejor tiempo observado |
| **M√°ximo** | 108,129 ms | Peor tiempo de respuesta (pico m√°ximo) |
| **% de Error** | 1.11 % | Se presentaron fallos leves en algunas peticiones |
| **Rendimiento (Throughput)** | 1.9 req/seg | Promedio de peticiones procesadas por segundo |
| **Kb/sec** | 1.04 KB/s | Promedio de transferencia de datos por segundo |

---

### Comportamiento del Sistema

- El sistema **soport√≥ el incremento r√°pido de usuarios hasta 100 concurrentes**, logrando procesar **casi 1,000 solicitudes** durante la prueba.
- Sin embargo, el **tiempo medio de respuesta (42 segundos)** es **significativamente alto**, indicando que el backend o infraestructura AWS **no escal√≥ eficientemente** ante el aumento de carga.
- La **dispersi√≥n entre m√≠nimo y m√°ximo (1.3 s a 108 s)** demuestra una **alta variabilidad** en el desempe√±o.
- Se observ√≥ un **peque√±o porcentaje de errores (1.11%)**, lo cual, aunque bajo, evidencia posibles l√≠mites de saturaci√≥n.
- El **rendimiento (1.9 req/seg)** muestra que el sistema proces√≥ peticiones constantemente, pero con tiempos cada vez m√°s lentos a medida que crec√≠a la concurrencia.

---

### Identificaci√≥n de Posibles Cuellos de Botella

1. **Tiempos de respuesta excesivos (Media: 42 s)**  
   Indican problemas de rendimiento en el procesamiento interno o falta de capacidad para manejar la carga concurrente.

2. **Desempe√±o inconsistente (Alta variabilidad)**  
   Podr√≠a deberse a:
   - Saturaci√≥n de recursos (CPU, memoria o base de datos).
   - Bloqueos en operaciones sincr√≥nicas.
   - Contenci√≥n en conexiones o hilos.

3. **Errores (1.11%)**  
   Probablemente relacionados con timeouts o l√≠mites del backend ante alta concurrencia.

4. **Posible falta de escalado autom√°tico (AWS)**  

---

### Recomendaciones de Mejora

1. **Optimizar rendimiento del backend**
   - Revisar endpoints con alta latencia (consultas, operaciones I/O).
   - Implementar **asincron√≠a**, **pool de conexiones** o **cach√©** en puntos cr√≠ticos.

2. **Ajustar infraestructura AWS**
   - Activar o mejorar **Auto Scaling** en infraestructura de aws.
   - Revisar configuraci√≥n de **load balancer** y tama√±o de instancias.
   - Verificar **tiempos de warm-up** o **cold start** si aplica.

3. **Optimizar manejo de concurrencia**
   - Usar colas (SQS, Kafka, RabbitMQ) para desacoplar operaciones pesadas.

4. **Repetir prueba con escenarios progresivos**
   - Ejecutar una **prueba de escalamiento gradual (Ramp slow)** para observar en qu√© punto comienza la degradaci√≥n.
   - Medir impacto del escalado horizontal o caching en los resultados.

---

### Conclusi√≥n

El sistema **responde bajo carga** y mantiene **disponibilidad general**, pero el **rendimiento se degrada severamente** con 100 usuarios concurrentes.  
Aunque el porcentaje de error es bajo (1.11%), los **tiempos de respuesta son demasiado altos (promedio de 42 s)**, lo que **no es aceptable para producci√≥n**.

El comportamiento evidencia **limitaciones de rendimiento o escalabilidad** en el backend o infraestructura AWS actual.

---

### üî¥ Estado Final del Sistema

| Estado | Descripci√≥n |
|--------|--------------|
| ‚ö†Ô∏è **Con Precauciones** | El sistema no falla, pero presenta tiempos de respuesta inaceptables bajo carga r√°pida. Se recomienda optimizaci√≥n y pruebas de escalabilidad antes de avanzar a entornos de producci√≥n. |

---

### Evidencias JMeter
### Configuracion inicial
![E2_configuracion_inicial_100](img/E2_configuracion_inicial_100.png)
### Reporte
![E2_reporte_100](img/E2_aws_reporte_100.png)

---

## Escenario 2: Escalamiento R√°pido (Ramp 200 Usuarios)

---

### Objetivo de la Prueba
Evaluar el comportamiento y la estabilidad del servicio desarrollado en **FastAPI** bajo condiciones de **aumento r√°pido de carga**, con el fin de determinar la capacidad del sistema para responder ante un incremento simult√°neo de usuarios concurrentes en un corto periodo de tiempo.

---

### Configuraci√≥n del Escenario
- **Tipo de prueba:** Escalamiento r√°pido (Ramp-up)  
- **Usuarios m√°ximos:** 200  
- **Periodo de subida:** 180 segundos (3 minutos)  
- **Duraci√≥n total:** 480 segundos (8 minutos)  
- **Framework:** FastAPI (Python)  
- **Comportamiento en error:** Configuraci√≥n espec√≠fica del test  

---

### M√©tricas Clave Obtenidas

| M√©trica | Valor |
|----------|--------|
| **# Muestras** | 751 |
| **Media (ms)** | 116,882 |
| **M√≠nimo (ms)** | 2,144 |
| **M√°ximo (ms)** | 246,331 |
| **Desviaci√≥n Est√°ndar** | 52,249.03 |
| **% Error** | 17.44% |
| **Rendimiento (Throughput)** | 1.4 req/seg |
| **Transferencia (KB/sec)** | 1.29 |
| **Media de Bytes** | 971.9 |
| **Tasa de env√≠o (Sent KB/sec)** | 3514.62 |

---

### Comportamiento del Sistema

Durante la prueba, el sistema mostr√≥ **una respuesta adecuada en las primeras fases del ramp-up**, pero al alcanzar la carga m√°xima de **200 usuarios concurrentes**, se evidenci√≥ un **incremento notable en la latencia** y un **n√∫mero significativo de errores (17.44%)**.  

El **tiempo de respuesta promedio (116,882 ms)** y la **alta desviaci√≥n est√°ndar (52,249 ms)** reflejan una **degradaci√≥n progresiva del rendimiento**, especialmente hacia el final del periodo de carga.  

---

### Identificaci√≥n de Posibles Cuellos de Botella

Se identifican los siguientes **puntos cr√≠ticos de desempe√±o**:
1. **Latencia acumulada:** Posible saturaci√≥n de los threads del servidor o del pool de conexiones.
2. **Errores en respuesta:** Indican que el servicio no logr√≥ atender correctamente todas las peticiones bajo carga m√°xima.
3. **Desviaci√≥n alta en tiempos de respuesta:** Sugiere ineficiencias en el manejo de solicitudes simult√°neas o bloqueos a nivel de E/S.
4. **Limitaci√≥n en el rendimiento sostenido:** Throughput bajo (1.4 req/seg) pese a la alta carga concurrente.

---

### Recomendaciones de Mejora

1. **Optimizar el servidor FastAPI:**
   - Aumentar el n√∫mero de *workers* y *threads* en la configuraci√≥n de Uvicorn/Gunicorn.  
   - Habilitar *keep-alive* y *connection pooling* para reducir overhead por conexi√≥n.

2. **Balanceo de carga:**
   - Implementar un **proxy inverso (NGINX o Traefik)** para distribuir las peticiones entre m√∫ltiples instancias.

3. **Estrategia de escalamiento gradual:**
   - Aumentar la rampa de subida (ramp-up) a 300-360 segundos para mitigar picos de saturaci√≥n inicial.

4. **Pruebas adicionales:**
   - Ejecutar una nueva iteraci√≥n con los ajustes propuestos para verificar si se estabiliza el rendimiento.

---

### Conclusi√≥n

El sistema **responde funcionalmente** bajo carga, pero presenta **limitaciones significativas en el manejo de concurrencia** y **degradaci√≥n del rendimiento** al alcanzar el pico de usuarios.  
El alto porcentaje de errores y la elevada variabilidad en tiempos de respuesta indican que **el sistema a√∫n no est√° completamente preparado para soportar un aumento abrupto de tr√°fico**.

---

### Estado Final
| Estado | Descripci√≥n |
|--------|--------------|
|**‚ö†Ô∏è En Precauci√≥n** | El sistema requiere optimizaci√≥n antes de considerarse estable bajo carga alta.|

---

### Evidencias JMeter
### Configuracion inicial
![E2_configuracion_inicial_200](img/E2_configuracion_inicial_200.png)
### Reporte
![E2_reporte_200](img/E2_aws_reporte_200.png)

---

## Escenario 2: Escalamiento R√°pido (Ramp 300 usuarios)

---

### **Objetivo de la Prueba**

Evaluar la **capacidad de respuesta y estabilidad del sistema** bajo un incremento r√°pido de usuarios concurrentes hasta 300, con el fin de identificar el punto en que el sistema comienza a degradar su rendimiento o generar errores.

---

### ‚öôÔ∏è **Configuraci√≥n del Escenario**

- **Tipo de prueba:** Prueba de escalamiento r√°pido (Ramp-up)  
- **Usuarios m√°ximos:** 300  
- **Periodo de subida:** 180 segundos (3 minutos)  
- **Duraci√≥n total:** 480 segundos (8 minutos)  
- **Framework:** FastAPI (Python)  
- **Comportamiento en error:** Configuraci√≥n espec√≠fica del test  

---

### üìä **M√©tricas Clave Obtenidas**

| M√©trica | Valor |
|----------|--------|
| **# Muestras** | 1,168 |
| **Media (ms)** | 115,966 |
| **Mediana (ms)** | 124,093 |
| **Percentil 90 (ms)** | 164,944 |
| **Percentil 95 (ms)** | 184,859 |
| **Percentil 99 (ms)** | 205,870 |
| **M√≠nimo (ms)** | 1,713 |
| **M√°ximo (ms)** | 222,432 |
| **% Error** | 14.55% |
| **Rendimiento (Throughput)** | 2.0 req/seg |
| **KB/seg (Recibido)** | 1.81 |
| **KB/seg (Enviado)** | 5412.04 |

---

### **Comportamiento del Sistema**

Durante el incremento progresivo hasta los **300 usuarios concurrentes**, el sistema mostr√≥ un **aumento considerable en los tiempos de respuesta**, alcanzando un promedio de **115.9 segundos**, lo que evidencia una **degradaci√≥n significativa del desempe√±o**.  
Los percentiles 95 y 99 superan los **180 segundos**, indicando que gran parte de las peticiones tardan demasiado o incluso podr√≠an estar generando timeouts.  

El **14.55% de error** es un indicador cr√≠tico, reflejando **fallos de conexi√≥n o respuestas incorrectas** bajo alta concurrencia.  
El throughput (2.0 req/seg) se mantuvo bajo, mostrando que el sistema no pudo procesar las solicitudes con eficiencia al aumentar la carga.

---

### **Identificaci√≥n de Posibles Cuellos de Botella**

- **Procesamiento en el backend (FastAPI):** tiempos de respuesta excesivos pueden indicar saturaci√≥n de hilos de ejecuci√≥n o bloqueos en operaciones s√≠ncronas.  
- **L√≠mites de concurrencia:** posible insuficiencia en la configuraci√≥n del servidor (n√∫mero de workers o threads).  
- **Base de datos o servicios externos:** posibles demoras en operaciones I/O.  
- **Infraestructura:** limitaciones de CPU, memoria o red frente a cargas intensas.  

---

### **Recomendaciones de Mejora**

1. **Optimizar el servidor FastAPI** ajustando los workers de Uvicorn/Gunicorn y revisando tareas bloqueantes.  
2. **Revisar los √≠ndices y tiempos de consulta** en la base de datos.  
3. **Analizar la necesidad de autoescalado** o balanceo de carga para distribuir la concurrencia.  
4. **Ejecutar una nueva prueba con una rampa de subida m√°s gradual** para detectar el umbral real de estabilidad.

---

### **Conclusi√≥n**

El sistema **no mantiene un desempe√±o adecuado bajo condiciones de escalamiento r√°pido**, mostrando **latencias elevadas y un porcentaje de error cr√≠tico (14.55%)**.  
Aunque responde parcialmente, su comportamiento indica una **saturaci√≥n temprana** y la necesidad de ajustes en configuraci√≥n, optimizaci√≥n y recursos.

---

### **Estado Final del Escenario**

| Estado | Descripci√≥n |
|---------|--------------|
| ‚ö†Ô∏è **Con Precauciones** | El sistema requiere optimizaciones antes de considerarse estable bajo cargas r√°pidas de 300 usuarios. Se recomienda revisar la infraestructura y configuraci√≥n del backend antes de volver a probar. |

---

### Evidencias JMeter
### Configuracion inicial
![E2_configuracion_inicial_300](img/E2_configuracion_inicial_300.png)
### Reporte
![E2_reporte_300](img/E2_aws_reporte_300.png)

---

## Escenario 3: Prueba de Sostenibilidad Corta

---

### **Objetivo de la Prueba**

Validar la **capacidad del sistema para mantener un rendimiento estable y consistente** durante una carga sostenida equivalente al **80% de la capacidad m√°xima (240 usuarios concurrentes)**.  
El objetivo es observar el comportamiento del sistema bajo una presi√≥n continua, identificar posibles degradaciones en el rendimiento y verificar la estabilidad de la API durante un periodo corto pero exigente.

---

### **Configuraci√≥n del Escenario**

- **Usuarios concurrentes:** 240 (80% de 300 usuarios)  
- **Periodo de subida:** 50 segundos  
- **Duraci√≥n total:** 300 segundos (5 minutos)  
- **Estrategia:** Carga sostenida al 80% de la capacidad m√°xima identificada  
- **Framework:** FastAPI (Python)

---

### **M√©tricas Clave Obtenidas**

| M√©trica | Valor |
|----------|-------|
| **# Muestras** | 746 |
| **Media (Avg Response Time)** | 103,218 ms |
| **Mediana** | 104,015 ms |
| **P90 (90% Line)** | 134,841 ms |
| **P95 (95% Line)** | 161,016 ms |
| **P99 (99% Line)** | 181,226 ms |
| **M√≠nimo (Min)** | 1,776 ms |
| **M√°ximo (Max)** | 197,379 ms |
| **% Error** | 12.87% |
| **Rendimiento (Throughput)** | 2.1 req/sec |
| **Kb/sec** | 1.75 |
| **Sent KB/sec** | 5,701.45 |

---

### **Comportamiento del Sistema**

Durante el periodo sostenido, el sistema mostr√≥ **estabilidad general** en la entrega de peticiones, manteniendo un rendimiento constante de **2.1 solicitudes por segundo**.  
Los tiempos de respuesta se mantuvieron en un rango **aceptable para carga sostenida**, con una media de **~103 segundos** y picos m√°ximos de hasta **197 segundos**.  
Se evidenci√≥ una ligera tendencia al aumento en la latencia hacia el final del escenario, indicando que el sistema comienza a alcanzar su umbral de saturaci√≥n, aunque sin presentar ca√≠das cr√≠ticas o interrupciones severas.

---

### **Identificaci√≥n de Posibles Cuellos de Botella**

- El **porcentaje de error del 12.87%** sugiere que algunas peticiones no fueron procesadas exitosamente, posiblemente debido a saturaci√≥n de recursos del servidor o tiempos de espera en las conexiones.
- La **latencia promedio alta (m√°s de 100 segundos)** podr√≠a estar asociada a limitaciones en la concurrencia del framework o a cuellos de botella en la base de datos.
- El **rendimiento limitado (2.1 req/sec)** muestra que la infraestructura a√∫n tiene oportunidades de optimizaci√≥n para manejar cargas sostenidas con mayor eficiencia.

---

### **Recomendaciones de Mejora**

1. **Optimizar consultas y endpoints cr√≠ticos** para reducir la latencia promedio.  
2. **Revisar configuraci√≥n de workers y pooling de conexiones** en el servidor FastAPI/Uvicorn.  
3. **Monitorear m√©tricas del sistema (CPU, RAM, I/O)** durante la ejecuci√≥n para confirmar la fuente de saturaci√≥n.  
4. Probar un escalamiento horizontal del servicio para evaluar mejoras en throughput.

---

### **Conclusi√≥n**

El sistema **mantiene estabilidad general y responde adecuadamente bajo una carga sostenida del 80%**, aunque con una **latencia considerablemente alta** y un **porcentaje de error relevante (12.87%)**.  
El rendimiento es constante, pero muestra indicios de saturaci√≥n a nivel de procesamiento interno.

---

### ‚úÖ **Estado Final**

| Estado | Descripci√≥n |
|---------|--------------|
|‚ö†Ô∏è **Resultado: En Precauci√≥n**  | El sistema **es funcional bajo carga sostenida**, pero requiere **ajustes de optimizaci√≥n y monitoreo adicional** antes de considerarse completamente estable para entornos productivos o de alta concurrencia.|

---

### Evidencias JMeter
### Configuracion inicial
![E3_configuracion_inicial](img/E3_configuracion_inicial.png)
### Reporte
![E3_reporte](img/E3_aws_reporte.png)