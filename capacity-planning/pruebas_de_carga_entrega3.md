# API
## RANKING
- **Metodo:** GET
- **Host:** labelb-2039041540.us-east-1.elb.amazonaws.com 
- **Api:** /api/videos 
- **URL** http://labelb-2039041540.us-east-1.elb.amazonaws.com/api/videos 

---

# ESCENARIO 1 (AWS,BALANCEADOR,AUTOSCALING) CAPACIDAD DE LA CAPA WEB 

# üß™ Informe de Pruebas Smoke Test

## ‚öôÔ∏è Configuraci√≥n de la Prueba
- **Herramienta:** Apache JMeter  
- **N√∫mero de Hilos:** 5  
- **Periodo de Subida:** 10 segundos  
- **Contador de Bucle:** Sin fin  
- **Duraci√≥n Total:** 60 segundos  
- **Infraestructura:** AWS (Balanceador de carga + AutoScaling + S3)  

---

## üßÆ 1. M√©tricas Clave Obtenidas

| M√©trica | Valor |
|----------|--------|
| **# Muestras (Requests)** | 2407 |
| **Tiempo Medio de Respuesta (Media)** | 115 ms |
| **Mediana (50%)** | 118 ms |
| **90 Percentil** | 139 ms |
| **95 Percentil** | 144 ms |
| **99 Percentil** | 161 ms |
| **Tiempo M√≠nimo** | 81 ms |
| **Tiempo M√°ximo** | 428 ms |
| **% de Error** | 50.02% |
| **Rendimiento (Throughput)** | 40.1 req/seg |
| **KB Recibidos/seg** | 21.84 |
| **KB Enviados/seg** | 11.74 |

---

## üìä 2. Comportamiento del Sistema

- El **tiempo medio de respuesta (115 ms)** indica un **buen desempe√±o individual de las peticiones exitosas**.  
- Los percentiles (90, 95 y 99) est√°n por debajo de 200 ms, mostrando **consistencia en las respuestas exitosas**.  
- Sin embargo, el **50.02% de errores** representa un **problema cr√≠tico de estabilidad o disponibilidad**, afectando significativamente el rendimiento global.  
- Esto sugiere que el sistema puede estar experimentando fallos en el balanceador, la capa de aplicaci√≥n o las dependencias externas (por ejemplo, S3).

---

## üö® 3. Identificaci√≥n de Posibles Cuellos de Botella

1. **Balanceador de Carga (ELB/ALB):**  
   - Posible configuraci√≥n inadecuada de distribuci√≥n o timeouts.  
   - Health checks agresivos podr√≠an causar desconexiones prematuras.

2. **Instancias de Aplicaci√≥n / AutoScaling:**  
   - El escalamiento autom√°tico podr√≠a estar reaccionando lentamente ante la carga.  
   - Revisar m√©tricas de **CPU, memoria y network I/O** en CloudWatch.

3. **Dependencias Externas (S3 / Servicios Internos):**  
   - Posible lentitud o errores en solicitudes hacia S3 u otros servicios backend.

4. **Log de error:**  
   - En la herramienta JMete vemos que en los logs que el error que arroja es 503

---

## üîß 4. Recomendaciones de Mejora
 
1. **Revisar m√©tricas de AWS CloudWatch:**  
   - `ELB 5XXErrorRate`, `TargetResponseTime`, `CPUUtilization`, `RequestCount`.  
2. **Ajustar pol√≠ticas de AutoScaling** para reducir el tiempo de reacci√≥n ante picos de carga.  
3. **Optimizar configuraci√≥n del balanceador de carga:**  
   - Ajustar `idle timeout`, `connection draining` y `health check thresholds`.  
4. **Ejecutar pruebas adicionales controladas:**  
   - Disminuir n√∫mero de hilos o duraci√≥n para comparar tasas de error.  
5. **Monitorear los logs del backend** para identificar excepciones o cuellos de conexi√≥n.

---

## üß© 5. Conclusi√≥n

El endpoint presenta **buen rendimiento en las peticiones exitosas (115 ms promedio)**, pero muestra **un alto nivel de errores (50%)**, lo que compromete su estabilidad.  
Esto indica que el sistema **no mantiene un comportamiento consistente bajo carga sostenida**, afectando su confiabilidad.

---

## üü° 6. Estado Final del Sistema

**Estado:** ‚ö†Ô∏è *Con Precauciones*  

> Aunque las respuestas exitosas son r√°pidas, la alta tasa de error impide aprobar la prueba.  
> Se recomienda **ajustar la configuraci√≥n de infraestructura (AutoScaling, Balanceador y dependencias)** antes de considerar este endpoint como estable para producci√≥n.

---

# üß™ Informe de Pruebas Ramp 100

## ‚öôÔ∏è Configuraci√≥n de la Prueba
- **Herramienta:** Apache JMeter  
- **N√∫mero de Hilos:** 100  
- **Periodo de Subida:** 180 segundos  
- **Contador de Bucle:** Sin fin  
- **Duraci√≥n Total:** 480 segundos  
- **Infraestructura:** AWS (Balanceador de Carga + AutoScaling + S3)  

---

## üßÆ 1. M√©tricas Clave Obtenidas

| M√©trica | Valor |
|----------|--------|
| **# Muestras (Requests)** | 339418 |
| **Tiempo Medio de Respuesta (Media)** | 114 ms |
| **Mediana (50%)** | 101 ms |
| **90 Percentil** | 118 ms |
| **95 Percentil** | 130 ms |
| **99 Percentil** | 194 ms |
| **Tiempo M√≠nimo** | 80 ms |
| **Tiempo M√°ximo** | 28,474 ms |
| **% de Error** | 96.01% |
| **Rendimiento (Throughput)** | 693.4 req/seg |
| **KB Recibidos/seg** | 431.87 |
| **KB Enviados/seg** | 203.14 |

---

## üìä 2. Comportamiento del Sistema

- El sistema proces√≥ un alto volumen de solicitudes (**m√°s de 339000 requests**) en 8 minutos, con un **throughput de 693 req/seg**, lo cual refleja buena capacidad de env√≠o desde el cliente.  
- Sin embargo, el **96% de las solicitudes fallaron** con error **HTTP 503 (Service Unavailable)**, lo que indica que el servicio no logr√≥ atender la carga solicitada.  
- El **tiempo medio de respuesta (114 ms)** de las pocas peticiones exitosas es bajo, se√±al de que cuando el sistema logra responder, lo hace con rapidez.  
- Los percentiles 90, 95 y 99 se mantienen en rangos bajos (‚â§ 200 ms), lo que refuerza la idea de que **el cuello de botella no est√° en el tiempo de respuesta, sino en la disponibilidad del servicio bajo carga**.

---

## üö® 3. Identificaci√≥n de Posibles Cuellos de Botella

1. **Balanceador de Carga (ELB/ALB):**  
   - Los errores 503 generalmente provienen del balanceador cuando **no hay instancias saludables disponibles** o **el target group se queda sin capacidad**.  
   - Puede deberse a timeouts, health checks mal configurados o falta de targets activos durante el test.

2. **AutoScaling / Capacidad de Instancias:**  
   - Posible **demora en la respuesta del autoscaling** ante el incremento de carga.  
   - Las instancias podr√≠an estar alcanzando el m√°ximo de conexiones simult√°neas o threads disponibles en el backend.

3. **Configuraci√≥n de L√≠mites del Servidor o API Gateway:**  
   - L√≠mites de concurrencia, pool de conexiones o configuraci√≥n de `max connections` pueden estar restringiendo la capacidad del servicio.

---

## üîß 4. Recomendaciones de Mejora

1. **Analizar en AWS CloudWatch:**
   - Revisar m√©tricas de **TargetResponseTime**, **5XXErrorRate**, **RequestCount**, **CPUUtilization**, y **HealthyHostCount**.  
   - Correlacionar el momento exacto del test con picos de error 503.

2. **Revisar configuraci√≥n del Balanceador:**
   - Ajustar **health checks** (thresholds y tiempos de espera).  
   - Evaluar aumentar el n√∫mero m√≠nimo de instancias activas en el grupo de AutoScaling.

3. **Optimizar el AutoScaling:**
   - Aumentar el tama√±o del *warm pool* o disminuir el tiempo de *cooldown* para que escale m√°s r√°pido.  
   - Evaluar el tama√±o de las instancias para asegurar capacidad suficiente de CPU y red.

4. **Revisar l√≠mites de aplicaci√≥n:**
   - Asegurar que el servidor backend tenga configurados l√≠mites adecuados de **threads**, **connection pools** y **timeouts**.

5. **Ejecutar una nueva prueba escalonada:**
   - Repetir la prueba con 50 hilos y 360 s de duraci√≥n para observar en qu√© punto comienzan los 503.  
   - Esto ayudar√° a determinar el **umbral real de saturaci√≥n del sistema**.

---

## üß© 5. Conclusi√≥n

El endpoint demuestra **excelente tiempo de respuesta en condiciones normales**, pero **falla masivamente bajo carga sostenida**, con **96% de errores 503**, lo que indica **incapacidad de escalar o mantener disponibilidad** durante picos de tr√°fico.  

El sistema requiere **ajustes de escalabilidad y capacidad en la capa de balanceo y aplicaci√≥n** antes de considerarse estable.

---

## üî¥ 6. Estado Final del Sistema

**Estado:** ‚ùå *Negado*  

> El servicio no supera la prueba de desempe√±o debido al alt√≠simo porcentaje de errores (96%).  
> Se recomienda realizar acciones correctivas en la infraestructura y volver a ejecutar las pruebas antes de avanzar hacia entornos de producci√≥n o pruebas de estr√©s m√°s amplias.

---

# üß™ Informe de Pruebas Ramp 200

## ‚öôÔ∏è Configuraci√≥n de la Prueba
- **Herramienta:** Apache JMeter  
- **N√∫mero de Hilos:** 200  
- **Periodo de Subida:** 180 segundos  
- **Contador de Bucle:** Sin fin  
- **Duraci√≥n Total:** 480 segundos  
- **Infraestructura:** AWS (Balanceador de Carga + AutoScaling)  

---

## üßÆ 1. M√©tricas Clave Obtenidas

| M√©trica | Valor |
|----------|--------|
| **# Muestras (Requests)** | 582141 |
| **Tiempo Medio de Respuesta (Media)** | 133 ms |
| **Tiempo M√≠nimo** | 78 ms |
| **Tiempo M√°ximo** | 42088 ms |
| **Desviaci√≥n Est√°ndar** | 654.69 ms |
| **% de Error** | 97.33% |
| **Rendimiento (Throughput)** | 1142.4 req/seg |
| **KB Recibidos/seg** | 716.84 |
| **KB Enviados/seg** | 334.69 |
| **Tama√±o Medio de Respuesta** | 642.5 bytes |

---

## üìä 2. Comportamiento del Sistema

- El sistema alcanz√≥ un **throughput muy alto (1142 req/seg)**, lo que indica que el generador de carga (JMeter) logr√≥ enviar grandes vol√∫menes de peticiones simult√°neas.  
- No obstante, el **97.33% de las peticiones fallaron** con error **HTTP 503 (Service Unavailable)**, reflejando **una falla cr√≠tica de disponibilidad** bajo alta concurrencia.  
- Las peticiones exitosas mantuvieron un **tiempo de respuesta promedio de 133 ms**, lo cual es excelente y muestra que, cuando el servicio logra responder, lo hace con eficiencia.  
- La **desviaci√≥n est√°ndar alta (654 ms)** y el **m√°ximo de 42 s** sugieren periodos intermitentes de congesti√≥n o saturaci√≥n de los recursos.  

En resumen:  
El sistema responde r√°pido en escenarios de baja carga, pero colapsa en disponibilidad cuando la demanda aumenta significativamente.

---

## üö® 3. Identificaci√≥n de Posibles Cuellos de Botella

1. **Balanceador de Carga (ELB/ALB):**  
   - Los errores **503** son t√≠picos de **falta de instancias saludables o saturaci√≥n del target group**.  
   - Podr√≠a estar alcanzando su l√≠mite de conexiones simult√°neas o presentando timeouts por backends no disponibles.

2. **AutoScaling:**  
   - El escalado autom√°tico puede no estar respondiendo con la suficiente rapidez.  
   - Si las pol√≠ticas de escalado dependen de CPU o memoria, el sistema podr√≠a estar escalando demasiado tarde frente al incremento de carga.

3. **Aplicaci√≥n Backend:**  
   - Limitaciones en el **pool de conexiones**, **timeouts internos** o **recursos compartidos** podr√≠an generar errores 503.  
   - Es posible que la aplicaci√≥n no soporte el n√∫mero de threads concurrentes generados por 200 hilos.

4. **Configuraciones de Red o L√≠mites de AWS:**  
   - Se debe revisar si existen l√≠mites de API Gateway, ALB, o throttling a nivel de VPC o instancia.  

---

## üîß 4. Recomendaciones de Mejora

1. **Monitoreo en AWS CloudWatch:**  
   - Revisar las m√©tricas de **5XXErrorRate**, **TargetResponseTime**, **HealthyHostCount**, y **CPUUtilization** durante el test.  
   - Validar si el AutoScaling se activ√≥ y cu√°ntas instancias se lanzaron.

2. **Ajustar pol√≠ticas de AutoScaling:**  
   - Configurar **warm pools** o **instancias preinicializadas** para evitar tiempos muertos en el escalado.  
   - Reducir el **cooldown** y utilizar m√©tricas proactivas (como n√∫mero de peticiones por target).

3. **Optimizar el balanceador (ALB):**  
   - Revisar **timeouts**, **connection draining** y **health check thresholds**.  
   - Aumentar el n√∫mero m√≠nimo de instancias activas en el grupo.

4. **Optimizaci√≥n del backend:**  
   - Revisar los l√≠mites de **thread pool**, **m√°ximo de conexiones simult√°neas**, y **par√°metros de timeout**.  
   - Implementar **cach√©s locales o distribu√≠das** para reducir carga sobre componentes cr√≠ticos.

5. **Reejecutar pruebas progresivas:**  
   - Ejecutar pruebas intermedias (por ejemplo, 100 hilos o 150 hilos) para identificar el **punto exacto de degradaci√≥n** del sistema.

---

## üß© 5. Conclusi√≥n

El endpoint presenta **buen rendimiento individual (133 ms promedio)**, pero **no mantiene estabilidad bajo alta concurrencia**.  
El **97% de errores 503** demuestran que el sistema **no est√° preparado para soportar 200 usuarios simult√°neos o m√°s** sin ajustar su escalabilidad y capacidad.  

La infraestructura **colapsa ante el incremento de carga**, evidenciando una limitaci√≥n en el balanceador o en la configuraci√≥n de escalado autom√°tico.

---

## üî¥ 6. Estado Final del Sistema

**Estado:** ‚ùå *Negado*  

> La prueba de desempe√±o no es aprobada debido al alto porcentaje de errores (97%).  
> Se requiere ajustar el AutoScaling, balanceador de carga y configuraci√≥n de backend antes de repetir pruebas de carga o considerar despliegue en producci√≥n.

---


# üß™ Informe de Pruebas Ramp 300

## üßÆ M√©tricas Clave Obtenidas

| M√©trica | Valor | Observaci√≥n |
|---------|-------|-------------|
| **Rendimiento** | 1,551.2 peticiones/segundo | Rendimiento consistente |
| **Throughput** | 973.68 KB/segundo | Capacidad de procesamiento de datos |
| **Throughput de Env√≠o** | 454.46 KB/segundo | Volumen de datos enviados |
| **Usuarios Concurrentes** | 300 hilos | Carga de usuarios simulados |
| **% Error** | 97,88% | Errores del sistema |
| **Rampa de Carga** | 180 segundos | Tiempo de escalado progresivo |
| **Duraci√≥n Total** | 480 segundos | Prueba de larga duraci√≥n |
| **Errores** | C√≥digo 503 | Service Unavailable |

## üìä Comportamiento del Sistema

### Aspectos Positivos
- **Rendimiento consistente**: Las m√©tricas muestran valores id√©nticos, indicando estabilidad durante la prueba
- **Capacidad de procesamiento**: 1,551 peticiones/segundo es un rendimiento considerable
- **Escalado progresivo**: Rampa de 180 segundos permite adaptaci√≥n gradual

### Problemas Identificados
- **Errores 503 cr√≠ticos**: Indican que el servicio no est√° disponible temporalmente
- **Posible saturaci√≥n**: Los errores 503 sugieren que el sistema alcanz√≥ su l√≠mite de capacidad

## üö® Identificaci√≥n de Posibles Cuellos de Botella

### 1. **Balanceador de Carga**
- Los errores 503 pueden indicar que el ALB no puede distribuir carga a instancias disponibles
- Posible l√≠mite de conexiones concurrentes alcanzado

### 2. **Auto Scaling Group**
- El escalado autom√°tico podr√≠a no estar respondiendo con suficiente rapidez
- Configuraci√≥n de pol√≠ticas de escalado posiblemente inadecuada

### 3. **L√≠mites de Servicio AWS**
- Posible alcanze de l√≠mites de EC2, ALB o otros servicios
- Throttling en servicios backend

### 4. **Recursos de Instancias**
- CPU, memoria o l√≠mites de red en las instancias EC2
- Configuraci√≥n insuficiente del tipo de instancia

## üîß Recomendaciones de Mejora

### **Inmediatas**
1. **Revisar configuraci√≥n de Auto Scaling**
   - Ajustar pol√≠ticas de escalado para respuesta m√°s r√°pida
   - Evaluar m√©tricas de CloudWatch para triggers de escalado

2. **Optimizar Balanceador de Carga**
   - Verificar health checks configuration
   - Revisar timeout y configuraci√≥n de idle connection

3. **Monitorear L√≠mites AWS**
   - Verificar Service Quotas para EC2 y ELB
   - Solicitar aumento de l√≠mites si es necesario

### **Medio Plazo**
1. **Optimizaci√≥n de Aplicaci√≥n**
   - Implementar circuit breakers
   - Mejorar manejo de conexiones de base de datos
   - Cachear respuestas cuando sea posible

2. **Arquitectura**
   - Considerar implementaci√≥n de CDN
   - Evaluar uso de AWS Lambda para picos de carga

## üß© Conclusi√≥n

La prueba revela que el sistema puede manejar una carga considerable (1,551 peticiones/segundo) pero presenta fallos cr√≠ticos bajo carga sostenida. Los errores 503 indican que la arquitectura actual no es capaz de mantener la disponibilidad del servicio bajo la carga objetivo de 300 usuarios concurrentes.

## **Estado Final del Sistema: üö´ NEGADO**

### Justificaci√≥n:
- **Presencia de errores 503** que afectan la disponibilidad del servicio
- **La capacidad actual no satisface** los requisitos de carga objetivo
- **Se requieren ajustes cr√≠ticos** en la configuraci√≥n de escalado y balanceo antes de considerar el sistema como production-ready

---

# üß™ Informe de Pruebas Sostenida Corta

## üßÆ M√©tricas Clave Obtenidas

| M√©trica | Valor | Observaci√≥n |
|---------|-------|-------------|
| **Usuarios Concurrentes** | 240 hilos | Carga de usuarios simulados |
| **Rampa de Carga** | 30 segundos | Escalado muy agresivo |
| **Duraci√≥n Total** | 300 segundos | Prueba de duraci√≥n media |
| **Configuraci√≥n** | Bucle sin fin | Carga continua hasta timeout |
| **Errores** | C√≥digo 503 | Service Unavailable |
| **M√©tricas Detalladas** | No disponibles | Falta informaci√≥n espec√≠fica de rendimiento |

## üìä Comportamiento del Sistema

### Problemas Cr√≠ticos Identificados
- **Errores 503 persistentes**: Service Unavailable en todas las peticiones
- **Rampa de carga muy agresiva**: 240 usuarios en 30 segundos (8 usuarios/segundo)
- **Fallo completo del sistema**: Incapacidad de procesar cualquier petici√≥n

### Comportamiento Esperado vs Real
- **Esperado**: Sistema deber√≠a manejar 240 usuarios con escalado progresivo
- **Real**: Colapso completo del servicio desde el inicio de la prueba

## üö® Identificaci√≥n de Posibles Cuellos de Botella

### 1. **Fallo en Auto Scaling Group**
- Las instancias posiblemente no est√°n disponibles o no pasan health checks
- Configuraci√≥n de Auto Scaling inadecuada para la carga inicial

### 2. **Balanceador de Carga Saturado**
- ALB posiblemente rechazando conexiones por falta de instancias backend sanas
- L√≠mites de conexiones concurrentes del ALB alcanzados

### 3. **Problemas de Configuraci√≥n**
- Health checks configurados incorrectamente
- Timeouts muy agresivos en el ALB
- Instancias no registradas correctamente en el target group

### 4. **Capacidad Insuficiente**
- Tipo de instancia inadecuado para la carga
- Recursos insuficientes (CPU, memoria, red)

## üîß Recomendaciones de Mejora

### **Urgentes (Antes de nuevas pruebas)**
1. **Verificar Estado del Auto Scaling Group**
   - Confirmar que las instancias est√°n en servicio
   - Revisar pol√≠ticas de health checks
   - Verificar configuraci√≥n de target groups

2. **Revisar Configuraci√≥n del ALB**
   - Health check path y thresholds
   - Timeout de conexi√≥n y inactividad
   - Configuraci√≥n de seguridad y grupos de seguridad

3. **Prueba de Humo Manual**
   - Verificar manualmente el endpoint
   - Confirmar que la aplicaci√≥n responde sin carga

### **Inmediatas**
1. **Ajustar Estrategia de Pruebas**
   - Reducir carga inicial (50 usuarios ‚Üí 100 ‚Üí 150 ‚Üí 240)
   - Aumentar rampa de carga (30s ‚Üí 120s)
   - Implementar pruebas escalonadas

2. **Monitoreo en Tiempo Real**
   - Configurar dashboards de CloudWatch
   - Monitorear m√©tricas de ALB y ASG durante pruebas

### **Medio Plazo**
1. **Optimizar Auto Scaling**
   - Ajustar pol√≠ticas basadas en m√©tricas personalizadas
   - Configurar warm-up periods
   - Implementar predictive scaling

## üß© Conclusi√≥n

El sistema presenta un **fallo catastr√≥fico** bajo carga, incapaz de procesar incluso una sola petici√≥n exitosamente. Los errores 503 consistentes indican problemas fundamentales en la configuraci√≥n de la infraestructura o en la disponibilidad del servicio backend.

## **Estado Final del Sistema: üö´ NEGADO CR√çTICO**

### Justificaci√≥n:
- **Fallo completo del servicio**: 100% de errores 503
- **Infraestructura no operativa**: El sistema no puede manejar carga alguna
- **Problemas de configuraci√≥n cr√≠ticos**: Evidencia de mala configuraci√≥n en ALB/ASG
- **Falta de resiliencia**: Sistema colapsa completamente ante carga m√≠nima

---

## üßÆ M√©tricas AWS

![t3_t2_CPUUtilization](img/t3_t2_CPUUtilization.jpg)
![t2NtwOut](img/t2NtwOut.jpg)
![t2NtwIn](img/t2NtwIn.jpg)

# An√°lisis de M√©tricas AWS CloudWatch

## üìä Resumen de M√©tricas Monitoreadas

### **CPU Utilization - Instancias t2.micro**
| Instancia | Zona de Disponibilidad | Uso M√°ximo de CPU | Patr√≥n de Comportamiento |
|-----------|------------------------|-------------------|--------------------------|
| **13.micro** | us-east-1c | ~10% | Pico sostenido alrededor de 10% |
| **12.micro** | us-east-1b | ~8% | Comportamiento estable en 6-8% |

### **Network Metrics - Instancias t2.micro**
| M√©trica | Tr√°fico M√°ximo | Patr√≥n de Comportamiento |
|---------|----------------|--------------------------|
| **Network In** | ~6.0 MB | Picos moderados, tr√°fico entrante consistente |
| **Network Out** | ~8.0 MB | Tr√°fico saliente mayor que entrante |

## üîç Comportamiento del Sistema

### **An√°lisis de CPU**
- **Baja utilizaci√≥n**: M√°ximo 10% en la instancia m√°s cargada
- **Estabilidad**: Las instancias mantienen uso constante sin picos abruptos
- **Capacidad ociosa**: Las instancias t2.micro tienen capacidad sobrante significativa

### **An√°lisis de Red**
- **Tr√°fico balanceado**: Distribuci√≥n entre us-east-1a y us-east-1b
- **Network Out > Network In**: T√≠pico en servicios que responden m√°s datos de los que reciben
- **Volumen moderado**: 6-8 MB indica carga de trabajo liviana

## üö® Identificaci√≥n de Posibles Cuellos de Botella

### **1. Problema Principal: No es la Infraestructura EC2**
- **Las instancias NO est√°n saturadas**: CPU < 10%, red estable
- **El cuello de botella est√° en otro lugar**: Posiblemente ALB, Auto Scaling, o aplicaci√≥n

### **2. Posibles Causas de Errores 503**
- **Balanceador de Carga (ALB)**: L√≠mites de conexiones o configuraci√≥n incorrecta
- **Auto Scaling Group**: Health checks fallando o configuraci√≥n err√≥nea
- **Aplicaci√≥n**: Timeouts internos o problemas de conexi√≥n a base de datos
- **L√≠mites de Servicio AWS**: Throttling en ALB u otros servicios

### **3. Evidencia de Infraestructura Sana**
- Las instancias EC2 est√°n saludables y con baja carga
- No hay problemas de red a nivel de instancia
- Las m√©tricas indican que las instancias podr√≠an manejar m√°s carga

## üìà Conclusi√≥n

### **Hallazgos Clave**
- ‚úÖ **Infraestructura EC2 estable**: CPU y red dentro de par√°metros normales
- ‚úÖ **Instancias no saturadas**: Capacidad sobrante significativa
- ‚ùå **Problema en capa de orquestaci√≥n**: ALB/Auto Scaling probable causa de 503
- ‚ùå **Configuraci√≥n posiblemente incorrecta**: Health checks o l√≠mites de servicio

# ESCENARIO 2 (LOCAL) RENDIMIENTO DE LA CAPA WORKER

# üß™ An√°lisis de resultados - Pruebas de procesamiento de videos

## üìã Tabla de resultados

| Tama√±o de videos | Cantidad de workers | Cantidad de videos inyectados | Tiempo en ejecutar la totalidad de mensajes | Tiempo en inyectar a la cola | Throughput / Min | Tiempo promedio video procesado (C/U) | √âXITO | FALLO |
|------------------|--------------------:|------------------------------:|--------------------------------------------:|------------------------------:|-----------------:|---------------------------------------:|-------:|-------:|
| 50MB | 1 | 50 | 10,88 s | 4,59 MEN/SEG | 373,91 | 8,02 VID/SEG | 50 | 0 |
| 50MB | 1 | 100 | 23,52 s | 4,25 MEN/SEG | 402,97 | 14,89 VID/SEG | 100 | 0 |
| 50MB | 2 | 50 | 14,47 s | 3,45 MEN/SEG | 1271,45 | 2,36 VID/SEG | 50 | 0 |
| 50MB | 2 | 100 | 28,05 s | 3,56 MEN/SEG | 1179,83 | 5,09 VID/SEG | 100 | 0 |
| 50MB | 4 | 50 | 17,2 s | 2,89 MEN/SEG | 2892,52 | 1,04 VID/SEG | 50 | 0 |
| 50MB | 4 | 100 | 33 s | 3,02 MEN/SEG | 6459,59 | 0,93 VID/SEG | 100 | 0 |
| 100MB | 1 | 50 | 13,8 s | 3,59 MEN/SEG | 404,07 | 7,42 VID/SEG | 50 | 0 |
| 100MB | 1 | 100 | 22,0 s | 4,5 MEN/SEG | 381,36 | 15,73 VID/SEG | 100 | 0 |
| 100MB | 2 | 50 | 13,3 s | 3,48 MEN/SEG | 1039,51 | 2,89 VID/SEG | 50 | 0 |
| 100MB | 2 | 100 | 29,2 s | 3,42 MEN/SEG | 1299,71 | 4,62 VID/SEG | 100 | 0 |
| 100MB | 4 | 50 | 16,5 s | 3,01 MEN/SEG | 3122,01 | 0,96 VID/SEG | 50 | 0 |
| 100MB | 4 | 100 | 34,1 s | 2,92 MEN/SEG | 6721,15 | 0,89 VID/SEG | 100 | 0 |

---

## ‚öôÔ∏è An√°lisis general

Se evalu√≥ el rendimiento del sistema variando:

- **Tama√±o del video:** 50 MB y 100 MB.  
- **N√∫mero de workers:** 1, 2 y 4.  
- **Cantidad de videos inyectados:** 50 y 100.  

Y se midieron m√©tricas clave como:
- Tiempo total de ejecuci√≥n.  
- Tasa de inyecci√≥n de mensajes.  
- Throughput (videos procesados por minuto).  
- Tiempo promedio por video.  
- √âxito y fallos.

---

## üìä Hallazgos principales

### 1Ô∏è‚É£ Efecto del n√∫mero de workers (paralelismo)
- El **throughput aumenta dr√°sticamente** al subir de 1 a 2 y especialmente a 4 workers, tanto para 50 MB como para 100 MB.  
  Ejemplo (50 MB, 100 mensajes):  
  - 1 worker ‚Üí **402,97 vid/min**  
  - 2 workers ‚Üí **1179,83 vid/min**  
  - 4 workers ‚Üí **6459,59 vid/min**

- El **tiempo promedio por video** disminuye de manera significativa:  
  - 1 worker: 14,89 s  
  - 4 workers: ~1 s  
  ‚û§ Se confirma una **distribuci√≥n eficiente de la carga** y buena **escalabilidad horizontal**.

---

### 2Ô∏è‚É£ Efecto del tama√±o del video (50 MB vs 100 MB)
- Al duplicar el tama√±o de los videos, el **throughput disminuye ligeramente**, sobre todo con menos workers.  
  Ejemplo (1 worker, 100 mensajes):  
  - 50 MB ‚Üí 402,97 vid/min  
  - 100 MB ‚Üí 381,36 vid/min  

- Con mayor paralelismo (4 workers), la diferencia entre tama√±os se reduce, indicando que **la concurrencia mitiga el impacto del tama√±o de archivo**.

---

### 3Ô∏è‚É£ Tiempo de inyecci√≥n a la cola
- Los valores de **‚ÄúTiempo en inyectar a la cola‚Äù** (entre 2,9 y 4,6 MEN/SEG) se mantienen **estables**, lo que sugiere que **el cuello de botella est√° en el procesamiento**, no en la inyecci√≥n.

---

### 4Ô∏è‚É£ √âxito y fallos
- En todas las pruebas se obtuvo un **100% de √©xito (sin fallos)**.  
  ‚û§ Esto valida la **robustez funcional** del sistema bajo diferentes configuraciones de carga.

---

## üìà Tendencias clave

| Variable | Tendencia observada | Interpretaci√≥n |
|-----------|--------------------|----------------|
| **Workers ‚Üë** | Throughput ‚Üë, Tiempo promedio ‚Üì | Escalabilidad efectiva |
| **Tama√±o video ‚Üë** | Throughput ‚Üì levemente | El tama√±o impacta, pero no limita el rendimiento |
| **Inyecci√≥n MEN/SEG** | Constante | No representa cuello de botella |
| **√âxito/Fallo** | 100% / 0% | Sistema estable y confiable |

---

## üß© Conclusi√≥n t√©cnica

> El sistema demuestra un comportamiento **altamente escalable, estable y eficiente** bajo distintos niveles de carga.  
> Aumentar la cantidad de workers mejora significativamente el throughput y reduce el tiempo de servicio por video.  
> El tama√±o del archivo afecta ligeramente el rendimiento, pero el paralelismo compensa la diferencia.  
> No se observaron fallos en ning√∫n escenario, evidenciando la **robustez del pipeline de procesamiento**.
